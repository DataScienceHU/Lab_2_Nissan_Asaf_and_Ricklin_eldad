---
title: "52414 - lab 2 "
author: "52414"
date: "25/5/2022"
output: html_document
---


# *Lab 2: Text analysis, Sampling and inference*  
    
<br/><br/>
The only allowed libraries are the following (**please do not add your own without consulting the course staff**):
```{r, include=FALSE}
library(tidyverse) # This includes dplyr, stringr, ggplot2, .. 
library(data.table)
library(ggthemes)
library(stringr)
library(tidytext) 
library(rvest)
```  
<br/><br/>

## Analysis of textual data and the `Wordle` game 
    
In this lab we will analyze textual data from the web. We will compute several statistics, and 
also implement and solve the popular game [wordle](https://en.wikipedia.org/wiki/Wordle).   

### General Guidance
- Your solution should be submitted as a full `Rmd` report integrating text, code, figures and tables. You should also submit the `html` file generated from it. 
For each question, describe first in the text of your solution what you're trying to do, then include the relevant code, 
then the results (e.g. figures/tables) and then a textual description of them. 

- In most questions the extraction/manipulation of relevant parts of the data-frame can be performed using commands from the `tidyverse` and `dplyr` R packages, such as `head`, `arrange`, `aggregate`, `group-by`, `filter`, `select`, `summaries`, `mutate` etc.

- When displaying tables, show the relevant columns and rows with meaningful names, and describe the results. 

- When displaying figures, make sure that the figure is clear to the reader, axis ranges are appropriate, labels for the axis , title and different curves/bars are displayed clearly (font sizes are large enough), a legend is shown when needed etc. 
Explain and describe in text what is shown in the figure. 

- In many cases, data are missing (e.g. `NA`). Make sure that all your calculations (e.g. taking the maximum, average, correlation etc.)
take this into account. Specifically, the calculations should ignore the missing values to allow us to compute the desired results for the rest of the values (for example, using the option `na.rm = TRUE`). 

- **Grading:** There are $17$ questions overall (plus a *bonus* sub-question). Each *sub-question* is worth $6$ points. (Total: $102$ points)


### Questions: 

#### PART 1 - MOBY-DICK

1.a. Load the complete `Moby dick`  book from the [Gutenberg project](https://www.gutenberg.org) into `R`. The book is available [here](https://www.gutenberg.org/files/2701/2701-h/2701-h.htm).
Extract the text from the html as a long string, and print the first line of the text in the file (starting with `The Project Gutenberg ...`)

```{r, cache = TRUE}
url <- 'https://www.gutenberg.org/files/2701/2701-h/2701-h.htm'
# Read the html into r:
html_str <- toString(html_text(read_html(url)))
as_string <- gsub("\r\n", " \n ", substr(html_str , 911, nchar(html_str)))
```
```{r}
unlist(strsplit(as_string, " \n "))[1]
```
b. Split the text string into words, separated by spaces, commas (`,`), periods (`.`), and new line characters (`\n` and `\r`). How many words are there? 
Compute the distribution of lengths of words you got, and plot using a bar-plot. What are the `median`, `mean`, `longest` and `most common` word lengths? <br>
**Note:** some of the "words" you will get will still contain non-English characters (e.g. numbers, `-`, `;` or other charactsers). Don't worry about it. We will parse the words further later when needed. 
```{r}

delete_not_char <- function(a, b=F){
  a <- str_replace_all(a,"[ \n ]", " ")
  a <- str_replace_all(a,"[^a-zA-Z ]", "") # refer to shortcut like "can't"
  if (b==T){a <- str_to_lower(a)}
  a
}

split_a <- function(a, b=F){ # a function that get function that gets string and give word list. 
  a <- delete_not_char(a, b)
  a <- unlist(strsplit(a, " ")[1])
 return(a[a!=""])
}

words <- split_a(as_string)
len_words <- nchar(words)
longest_words <- words[which.max(len_words)]
median_words <- median(len_words)
mean_words <- mean(len_words)
t_len_words <- table(len_words)
most_common <- names(t_len_words[order(t_len_words, decreasing = T)][1])

```
longest words: ``r longest_words``

median words: ``r median_words``

mean words: ``r mean_words``

most common: ``r most_common`` 

c. Count the words frequencies in the text - i.e. the number of times each unique word appears in the text.
Show the top 10 most frequent words with their frequencies. Is the list of top words surprising? explain. 

```{r}
t_words <- table(words)
t_words[order(t_words, decreasing = T)][1:10]
```
not that surprising, really common-daily words.

2.a. Split the book text into `chapters`, such that you obtain an array of strings, one per chapter. 
Count and plot the number of words per each chapter (y-axis) vs. the chapter index (1,2,3.., on x-axis). 
(each chapter is splitted to word in the same manner as in Qu. 1). <br>
**Hint:** Chapters begin with the string `CHAPTER` followed by a space and then the chapter's number and a period. For example: `CHAPTER 2.` is at the start of chapter 2. But beware - this pattern by itself is not enough to identify correctly all chapters starts and end. You will need to *look at the text* in order to decide what patterns to look for when splitting the data into chapters. 

```{r}
begining <- str_locate_all( as_string,"CHAPTER 1. Loomings")[[1]][2,1]
end <- str_locate_all( as_string,"only found another orphan.")[[1]][1,2]
book <- str_sub(as_string, begining, end)
book <- delete_not_char (book) # includ CHAPTER name
by_chepter <- str_split(book, "      CHAPTER  ")[[1]]
n_chepter <- length(by_chepter)
word_n_by_chepter <- rep(0,n_chepter)
for (i in 1:n_chepter){
  word_n_by_chepter[i] <- length(split_a(by_chepter[i]))
}

word_n_by_chepter[1] - 1 # without the word "CHAPTER"
#add plot vechi
```
b. Write a function that receives as input a query word, and an array of strings representing the chapters. The function returns a vector of the `relative frequencies` of the word in each chapter. That is, if for example the word `time` appears six times in the first chapter, and there are overall 
$3247$ words in this chapter, then the first entry in the output vector of the function will be $6/3247 \approx 0.0018$. <br>
Apply the function to the following words `Ahab`, `Moby`, `sea`. Plot for each one of them the trend, i.e. the frequency vs. chapter, with chapters in increasing orders. Do you see a different behavior for the different words? in which parts of the book are they frequent? 

```{r}
get_frq <- function(string_word, by_chepter){
    output <- rep(0,length(by_chepter))
    for (i in 1:length(by_chepter)){
      a <- table( split_a(by_chepter[i]) )
      if (string_word %in% names(a)){
        output[i] <- as.numeric(a[names(a) == string_word]) / word_n_by_chepter[i]
        }
    }
output
}
#get_frq('Ahab',by_chepter)
#get_frq('Moby',by_chepter)
#get_frq('sea',by_chepter)

```

3.a. Suppose that Alice and Bob each choose independently and uniformly at random a single word from the book. That is, each of them chooses a random word instance from the book, taking into account that words have different frequencies (i.e. if for example the word `the` appears $1000$ times, and the word `Ishmael` appears only once, then it is $1000$-times more likely to choose the word `the` because each of its instances can be chosen). What is the probability that they will pick the same word? 
Answer in two ways: <br>
(i) Derive an exact formula for this probability as a function of the words relative frequencies, and compute the resulting value for the word frequencies you got for the book. <br>

```{r}
relative_frequencies <- sum(t_words^2)/(sum((t_words))^2)
sum(relative_frequencies)
```

(ii) Simulate $B=100,000$ times the choice of Alice and Bob and use these simulations to estimate the probability that they chose the same word. <br>
Explain your calculations in both ways and compare the results. Are they close to each other? 

```{r}
book_vector <- split_a(book)
n_words <- length(book_vector)
indicator <- 0
a <- sample(1:n_words, size = 100000)
b <- sample(1:n_words, size = 100000)
c <- as.numeric(book_vector[a] == book_vector[b])
sum(c) / 100000
```

b. Suppose that instead, we took all **unique** words that appear in the book, and then Alice and Bob would choose each independently and uniformly at random a single word from the list of unique words. What would be the probability that they chose the same word in this case? is it lower, the same, or higher then the probability in (a.)? explain why. 

```{r}
t_words <- table(book_vector )
o_t_words <- order(t_words, decreasing = T)
not_unique <- 1 / length(t_words)
```
not unique, when all word is value 1, `r not_unique ` is lower

only by knowing first sum of ratio:
```{r} 
sum((o_t_words[1:3]/length(t_words))) 
```

\begin{equation}
P(not\ unique) =\frac{1}{n} \\
< \frac{1.1}{n}  = \frac{(1.4*n)^2}{n^2} < \frac{\sum^{n}a_{[1:3]_{i}^2}}{n^2} < \\ \frac{\sum^{n} (a_{i}^2)}{(\sum{a})^2} = P(unique)
\<end{equation}

4.a. Extract from the book a list of all `five-letter` words. Keep only words that have only English letters. Convert all to lower-case. How many words are you left with? how many unique words? 
Show the top 10 most frequent five-letter words with their frequencies.

```{r}
book_vector_b <- split_a(book, b=T)
only_5 <- book_vector_b[nchar(book_vector_b)==5]

t_words_b <- table(only_5)
o_t_b_words <- t_words_b[order(t_words_b, decreasing = T)][1:10]

o_t_b_words 
length(t_words_b)
length(only_5)
```

b. Compute letter frequencies statistics of the five-letter words: 
That is, for each of the five locations in the word (first, second,..), how many times each of the English letters `a`, `b`,...,`z` appeared in your (non-unique) list of words. Store the result in a `26-by-5` table and show it as a heatmap. Which letter is most common in each location? Do you see a strong effect for the location? 

```{r}
words_5b <- names(t_words_b)
b4 <- function(words_5letters, heat = T){
out_met <- matrix(data = rep(0, 26*5), nrow = 26, ncol = 5)
for (i in 1:26){
  for (j in words_5letters){
    a = str_locate_all (j , letters[i])[[1]][,1]
  out_met[i, a] = out_met[i,a] + 1
  }
}
rownames(out_met) <- letters
if(heat){
out_heat <- heatmap(out_met,  Colv = NA, Rowv = NA,cexRow = 0.5,cexCol = 0.5, scale="column")
return(list(out_met,out_heat))} 
return(out_met)
}

b4_a <- b4(words_5b)
five_matrix <- b4_a[[1]]
```



c. Consider the following random model for typing words: we have a `26-by-5` table of probabilities $p_{ij}$ for i from $1$ to $5$, 
and $j$ going over all $26$ possible English letters (assuming lower-case). (This table stores the parameters of the model).
Here, $p_{ij}$ is the probability that the $i$-th letter in the word will be the character $j$. 
Now, each letter $i$ is typed from a categorical distribution over the $26$ letters, with probability $p_{ij}$ of being the character $j$, and the letters are drawn independently for different values of $i$. 
For example,  using $p_{5s}=0.3$ we will draw words such that the probability of the last character being `s` will be $0.3$. <br>
For each five-letter word $w$, its likelihood under this model is defined simply as the probability of observing this word when drawing a word according to this model, that is, if $w=(w_1,w_2,w_3,w_4,w_5)$ with $w_i$ denoting the $i$-th letter, then $Like(w ; p) = \prod_{i=1}^5 p_{i w_i}$. <br>
Write a function that receives a `26-by-5` table of probabilities and an array of words (strings), and computes the likelihood of each word according to this model. <br>
Run the function to compute the likelihood of all unique five-letter words from the book, with the frequency table you computed in 4.b. normalized to probabilities. Show the top-10 words with the highest likelihood. 

```{r}
table_prob <- function(tble, arr){
  prob <- tble/sum(tble)
  g <- rep(0,length(arr))
  for (j in 1:length(g)){
    a = 1
    word_a <- strsplit(arr[j],'')[[1]]
  for (i in 1:length(word_a)){
    a <- a * prob[c(word_a[i]),i]
  }
  g[j] <- a
  }
  return(arr[order(g, decreasing = T)])
}
table_prob(five_matrix,names(t_words_b))[1:10]
```

because we considered for each word "word", possessive "word's" and plural "words", as single words, s at the end was very common.

#### PART 2 - WORDLE

In `wordle`, the goal is to guess an unknown five-letter English word. At each turn, we guess a word, and get the following feedback: the locations at which our guess matches the unknown word (`correct`), the locations at which our guess has a letter that appears in the unknown word but in a different location (`wrong`), and the locations at which our guess contains a letter that is not present in the unknown word (`miss`).

We supply to you a function called `wordle_match`, that receives as input a guess word and the true word (two strings), and returns an array of the same length indicating if there was a `correct` match (1), a match in the `wrong` location (-1), or a `miss` (0). For example: calling `match_words("honey", "bunny")` will yield the array: `[0, 0, 1, 0, 1]`, whereas calling `match_words("maple", "syrup")` will yield the array `[0, 0, -1, 0, 0]`. 

**Note:** It is allowed for both the unknown word and the guess word to contain the same letter twice or more. In that case, we treat each letter in the guess as a `wrong` match if the same letter appears elsewhere in the unknown word. This is a bit different from the rules of the `wordle` game and is used for simplification here. 

```{r}
match_words <- function(guess,tru){
  tru <- strsplit(tru,'')[[1]]
  guess <- strsplit(guess,'')[[1]]
  output <- rep(0,5)
  for (i in 1:5){
    if (tru[i] == guess[i]){ output[i] <- 1 } 
    else if (guess[i] %in% tru) { output[i] <- -1 }
  }
  return(output)
}
match_words("maple", "syrup")
```

5.a. Download the list of five-letter words from [here](https://www-cs-faculty.stanford.edu/~knuth/sgb-words.txt). This list contains most common English five-letter words (each word appears once).  
Compute and display the `26-by-5` table of frequencies for this word list, in similar to Qu. 4.b.
Do you see major differences between the two tables? why? 

```{r}
wordling <- paste(readLines("C:/Users/asafm/OneDrive/docs/2122B/Ranal/sgb-words.txt"))
b4(wordling)
```
b. Write a function that receives an array of guess words, an array of their corresponding matches to the unknown word (i.e. a two-dimensional array), and a `disctionary` - i.e. an array of legal English words. 
The function should return all the words in the dictionary that are consistent with the results of the previous guesses. For example, if we guessed `maple` and our match was the array `[1, 0, -1, 0, 0]`, then we should keep only words that start with an `m`, have a `p` at a location different from $3$, and don't have `a`, `l` and `e`.
When we have multiple guesses, our list of consistent words should be consistent with all of them, hence as we add more guesses, the list of consistent words will become shorter and shorter. <br>
Run your function on the list of words from (a.), and with the guesses `c("south", "north")` and their corresponding matches: `c(-1, 1, 1, 0, 0)` and `c(0, 1, 0, 0, 0)`. Output the list of consistent words with these two guesses. 

```{r}

b5 <- function(array_guess , array_matches, disctionary){
    # takes array and gstr_locate_all('aba', 'a')[[1]][,1]ame-responds and dedact posible words in a certain dictionary
  certain <- rep('.', 5) # in the place
  nothing <- c() # not in any place
  certain_not <- rep('.',5) # not in specific place
  total <- c() # all possible words
  for (i in 1:5){
    col = array_matches[,i]
    w <- which(0 == col)
    if (length(w) != 0){
      nothing <- unique(append(nothing, str_sub(array_guess[w],i,i)))}
    w <- which(-1 == col)
    if (length(w) != 0){
      total <- unique(append(total, str_sub(array_guess[w],i,i)))}
    if (1 %in% col) {
      certain[i] <- str_sub(array_guess[which(1 == col)[1]],i,i)}
    else if (length(w) != 0){
      certain_not[i] <- paste('[^(',paste(unique(str_sub(array_guess[w],i,i)),collapse = '|'),')]')}
  }
  correct = sum(as.numeric(certain != '.'))
  if (length (total) != 0){
    total = total[!total %in% certain[certain != '.']]
    total_n = length(total)}
    output <- c()

  if (correct == 5){return(output)}
  for (i in c(1:5)[certain == '.']){ 
    if (certain_not[i] != '.'){certain[i] <- certain_not[i]}}
  certain <- paste(certain, collapse = '' )
  for (i in disctionary ) {
    if (length(str_which( i ,nothing)) == 0){ output <- append(output, i)}
  } # all contain miss cannot be the answer
  output <- output[str_detect(output,certain)]
  if (length (total) != 0){
  for (i in output){
    if(length(str_which(i,total)) < total_n){ output <-output[output!=i]}
  }
  }
  return(output)
}
a_g = c("south", "north")
a_m = matrix(c(-1, 1, 1, 0, 0, 0, 1, 0, 0, 0),nrow = 2,byrow = T)
b5(a_g,a_m,wordling)

#validate!!!
```

6.a. Consider the following (rather naive) guessing strategy, called **strategy 1:** <br>
- We start with a random word with each letter sampled uniformly and independently from the $26$ English letters. 
- Then, at each turn, we look only at the previous perfect matches (`correct`) to the target word, and ignore matches at the `wrong` location and missing letters. At each place where there is a correct match, we use the correct letter, and at all other locations we keep sampling uniformly from the $26$ letters. We keep going until we get all the five letters correctly (and hence the word). 

We are interested in the number of turns (guesses) needed until we get the correct word. 

Implement a function that receives as input the unknown word, and implements this strategy. The output should be the number of turns it took to guess the word. The function should also record and print guess at each turn, as well as the match array , until the word is guessed correctly.  
Run the function when the unknown word is "mouse", and show the results. 

```{r}
a6 <- function(word_true, p = T){ #implament strtegy, can by without printing as p = F
  guess <- sample(letters, 5, replace = T)
  guess <- paste(guess, collapse = "")
  response <- match_words(word_true, guess)
  times <- 1
  guesses <- c(guess)
  while(!all(response == c(1,1,1,1,1))) {
    guess <- strsplit(guess,'')[[1]]
    guess[response != 1] <- sample(letters, length(guess[response != 1]), replace = T)
    guess <- paste(guess, collapse = "")
    response <- match_words(word_true, guess)
    guesses <- append(guesses, guess)
    times <- times + 1 
  }
  output <- list(times, guesses)
  if(p == T){
  print(output)} else {
  return(output) } 
}
a6('mouse')
```

b. Write a mathematical formula for the distribution of the number of turns needed to guess the target word with this strategy. <br> 
**Hint:** The geometric distribution plays a role here. It is easier to compute the cumulative distribution function.  
Use this formula to compute the expected number of turns for this strategy.

$P(n) = \sum_{1=i}^{5}\binom{n}{i}(1-p)^{ni}(p)^i(1-(1-p)^n)^{5-i}$
sum any combination of after n times, the last are geom in n, and the other are geom before n.

<br>
**Note:** The distribution has an infinite support (any positive number of turns has a positive probability), but high number of turns are very rare - you can neglect numbers above $10,000$ when computing the expectation. 

c. Compute empirically the distribution of the number of turns using the following Monte-Carlo simulation:
- Draw $B=100$ random unknown words, uniformly at random from the list of five-letter words in Qu. 5. 
- For each unknown word, run the guessing strategy implemented in (a.) and record the number of turns 
- Compute the average number of turns across all $B=100$ simulations. <br>
Plot the empirical CDF along with the theoretical CDF from (b.) on the same plot. Do they match? 
compare also the empirical expectation with the expectation computed in (b.). How close are they? 

```{r}
sample_words <- sample(wordling,100)
n_mc <- rep(0,100) # in monte carlo method
for (i in 1:100){
  n_mc[i] <- a6(sample_words[i], p = F)[[1]]
}
plot(ecdf(n_mc))

p <- 1 / 26
p_dt <- rep(0,10000) # probability deterministic 
cdf_dt <- rep(0,10000)
cdf_p <- 0
for (n in 1:10000){
  p1 <- 0
  for (i in 1:5)
  {p1 <- p1 + choose(5,i)*(1-p)^(n*i)*(p^i)*(1-(1-p)^n)^(5-i)}
  p_dt[n] <- p1
  cdf_p <- cdf_p + p1
  cdf_dt[n] <- cdf_p
}
plot(1:10000, cdf_dt)
plot(1:200, cdf_dt[1:200])

plot(ecdf(n_mc))
lines(1:200, cdf_dt[1:200], type = "l")

```

7.a. Implement the following two additional strategies for guessing the word: 

**Strategy 2:** 
- At each stage, we guess the word with the highest likelihood (see Qu. 4.c.), **of the remaining words that are consistent with the previous guesses**. 
- We keep guessing until obtaining the correct word. 


```{r}
st2 <- function(words , word_true){
  guess <- table_prob(b4(words,F),words)[1] # take the most MLE
  times <- 1
  guesses <- c(guess)
  response <- match_words(guess, word_true)
  mat_st2 <- matrix(response, nrow = 1) # add to game's response matrix
    while(!all(response == c(1,1,1,1,1))) {
      if (is.null(guess)){ return('invalid word')} # check whether there's possible words
      words <- b5(guesses,mat_st2,words) # consistant
      guess <- table_prob(b4(words,F),words)[1] # take the most MLE 
      response <- match_words(guess, word_true)
      mat_st2 <- rbind(mat_st2,response) # add to game's response matrix
      guesses <- append(guesses, guess)
      times <- times + 1 
}
  output <- list(times, guesses)
  return(output) 
}
st2( words = wordling,word_true ="vizor")

```

**Strategy 3:** 
The same as strategy 2, but at each stage we guess a random word sampled uniformly from all remaining consistent words (instead of guessing the word with the highest likelihood).

Run both strategies with the unknown word "mouse", and show the guesses and the number of turns for them, in similar to Qu. 6.a.



```{r}
st3 <- function(words , word_true){
  guess <- sample(words,1) # take random
  times <- 1
  guesses <- c(guess)
  response <- match_words(guess, word_true)
  mat_st2 <- matrix(response, nrow = 1) # add to game's response matrix
    while(!all(response == c(1,1,1,1,1))) {
      if (is.null(guess)){ return('invalid word')} # check whether there's possible words
      words <- b5(guesses,mat_st2,words) # consistant
      guess <- sample(words,1) # take random 
      response <- match_words(guess, word_true)
      mat_st2 <- rbind(mat_st2,response) # add to game's response matrix
      guesses <- append(guesses, guess)
      times <- times + 1 
}
  output <- list(times, guesses)
  return(output) 
}
st3( words = wordling,word_true ="mouse")
st2( words = wordling,word_true ="mouse")


```
b. Run $B = 100$ simulations of the games, in similar to Qu. 6.c. 
That is, each time, sample a random unknown word,  run the two strategies $2$ and $3$, and record the number of turns needed to solve `wordle` for both of them. 

```{r}
n_st2 <- rep(0,100)
n_st3 <- rep(0,100)

sample_words <- sample(wordling,100)

for (i in 1:100){
  n_st3[i] <- st3( words = wordling,word_true = sample_words[i])[[1]]
  n_st2[i] <- st2( words = wordling,word_true = sample_words[i])[[1]]
}

plot(ecdf(n_st3))
lines(ecdf(n_st2),col = 'red')
```

- Plot the empirical CDFs of the number of guesses. How similar are they to each other? how similar are they to the CDF of strategy 1? What are the empirical means for both strategies?  


c. (Bonus**) Can you design a better guessing strategy? 
Design and implement a different guessing strategy, run it on $B=100$ random simulations, show the empirical CDF and compute the empirical mean. Your strategy is considered `better` if it shows a significant reduction in the mean number of turns compared to the previous strategies (you should think how to show that the difference is significant)


**Solution:**  

[INSERT YOUR TEXT, CODE, PLOTS AND TABLE HERE, SEPERATED INTO SUB-QUESTIONS]


```{r, cache=TRUE}
# Helper function: 
wordle_match <- function(guess, word)  # 1: correct location, -1: wrong location, 0: missing
{
  L <- nchar(guess)
  match <- rep(0, L)
  for(i in 1:L)
  {
    if(grepl(substr(guess, i, i), word, fixed=TRUE))
      {match[i] = -1}
    if(substr(guess, i, i) == substr(word, i, i))
    {      match[i] = 1}
  }
  
  return(match)
}

```


